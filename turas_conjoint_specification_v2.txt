Turas Conjoint Analysis ModuleTechnical Specification for ImplementationVersion 2.0 | December 20251. Executive SummaryThis specification defines the requirements for upgrading the Turas Conjoint Analysis module from its current state to a production-ready system capable of handling real-world market research projects. The module must process Alchemer choice-based conjoint exports, perform statistically rigorous analysis using mlogit, and produce client-ready Excel outputs including an interactive market simulator.1.1 Current State AssessmentThe existing implementation has a sound architectural foundation but critical gaps prevent practical use:¥ Uses survival::clogit instead of the specified mlogit package¥ Cannot process native Alchemer export format (requires manual data transformation)¥ Output limited to 4 basic sheets without market simulator¥ No handling for 'None' option in choice sets¥ No segment-level analysis capability1.2 Target StateA complete conjoint analysis system that:¥ Directly ingests Alchemer CBC exports without preprocessing¥ Uses mlogit as primary estimation engine with clogit fallback¥ Produces 8-sheet Excel output with interactive market simulator¥ Handles all Alchemer choice types (Single Choice, Single Choice with None)¥ Delivers results statistically equivalent to commercial tools like Sawtooth2. Data Format Specifications2.1 Alchemer Export FormatAlchemer exports choice-based conjoint data with the following structure:ColumnExample ValueDescriptionResponseID89Unique respondent identifierSetNumber1, 2, 3, 4, 5Choice task number (typically 4-8 sets)CardNumber1, 2, 3Alternative within choice set[Attributes]High_107, MSG_PresentAttribute level shown (format varies)Score0 or 1 (or 100)1/100 = chosen, 0 = not chosen2.2 Attribute Level Naming ConventionsAlchemer encodes attribute levels in various formats that must be handled:¥ Price: 'Low_071', 'Mid_089', 'High_107' (with embedded numeric values)¥ Binary attributes: 'MSG_Present', 'MSG_Absent'¥ Multi-level: 'A', 'B', 'C', 'D', 'E' (for NutriScore)¥ Descriptive: 'Salt_Normal', 'Salt_Reduced'2.3 Required Data TransformerCreate new file: 05_alchemer_import.RThe transformer must:1. Create unique choice_set_id by combining ResponseID and SetNumber2. Normalize Score column (convert 100 to 1 if needed)3. Extract clean level names from compound values (e.g., 'Low' from 'Low_071')4. Handle None option rows if present (Choice Type: Single Choice with None)5. Validate data integrity (one choice per set, all attributes present)3. Analysis Engine Specification3.1 Primary Engine: mlogitReplace current clogit implementation with mlogit for proper discrete choice modeling.3.1.1 Why mlogit over clogit¥ Purpose-built for discrete choice analysis (not a Cox regression workaround)¥ Native support for panel data structure (repeated choices per respondent)¥ Extensible to mixed logit, nested logit for future enhancements¥ Better diagnostics and hypothesis testing capabilities¥ Industry standard in academic and commercial conjoint software3.1.2 Implementation RequirementsData preparation using mlogit.data():mlogit_data <- mlogit.data(df, choice = 'chosen',                          shape = 'long',                          alt.var = 'alternative_id',                          chid.var = 'choice_set_id',                          id.var = 'respondent_id')Model estimation:model <- mlogit(chosen ~ Price + Brand + ... | 0, data = mlogit_data)The '| 0' suppresses alternative-specific constants (standard for generic attributes).3.1.3 Fallback: survival::clogitRetain clogit as fallback when mlogit fails or for compatibility testing. Log which engine was used in output.3.2 None Option HandlingFor 'Single Choice with None' designs, the None alternative requires special treatment:¥ Include as a separate alternative in each choice set¥ Code with constant attributes (all zeros for dummy variables)¥ Estimate alternative-specific constant for None option¥ Interpret None constant as base utility of 'not choosing'3.3 Model DiagnosticsReport the following fit statistics:StatisticPurposeMcFadden R?Model fit (0.2-0.4 considered good for choice models)Hit RatePrediction accuracy (% correctly predicted choices)Log-LikelihoodModel fit measure for comparisonAIC/BICModel selection criteriaN observationsTotal rows in analysisN choice setsTotal choice tasksN respondentsSample size4. Excel Output Specification4.1 Eight-Sheet StructureThe output workbook must contain exactly these sheets in this order:#Sheet NameContents1Market SimulatorInteractive what-if analysis tool (primary deliverable)2Attribute ImportanceRanked importance scores with visualization data3Part-Worth UtilitiesZero-centered utilities by attribute and level4Utility Chart DataPre-formatted data for Excel charting5Model FitDiagnostic statistics and quality metrics6ConfigurationStudy design summary7Raw CoefficientsUncentered model coefficients with std errors8Data SummaryResponse counts, completion rates, data quality4.2 Market Simulator Sheet (Critical)This is the centerpiece deliverable. Reference the existing Excel simulator in Screenshot_20251126_at_22_24_13.png for layout guidance.4.2.1 Structure¥ Header section: Dataset name, study metadata¥ Product configuration area: Rows for up to 10 products¥ Attribute columns: Dropdowns for each attribute level¥ Utility calculation column: Sum of selected level utilities¥ Share calculation: Exp(utility) / Sum(Exp(utilities))¥ Visualization: Pie chart of estimated shares¥ Utility reference panel: All utilities displayed for reference4.2.2 Formula ImplementationFor each product row, implement:Total_Utility = SUMPRODUCT(selected_levels, utility_lookup)Exp_Utility = EXP(Total_Utility)Market_Share = Exp_Utility / SUM(all_Exp_Utilities)Use Excel Data Validation for attribute level dropdowns, referencing the utility lookup table.4.3 Professional FormattingApply consistent formatting throughout:¥ Header row: Dark blue background (#1F4E79), white bold text¥ Alternating row colors for data tables¥ Numeric formatting: 2 decimal places for utilities, 1% for shares¥ Column auto-width with minimum widths¥ Freeze panes on header rows¥ Print area and page setup configured5. Configuration System5.1 Enhanced Settings SheetExtend the Settings sheet to support Alchemer imports:SettingPurposedata_source'alchemer' or 'generic' - triggers appropriate importchoice_type'single', 'single_with_none', 'best_worst'none_option_labelLabel for None option in simulator (if applicable)estimation_engine'mlogit' (default), 'clogit', or 'auto'zero_centerTRUE/FALSE - whether to zero-center utilitiesbase_level_method'first', 'last', or 'effects' coding5.2 Attribute Sheet EnhancementAdd columns to support Alchemer format mapping:¥ AlchemerColumnName: Original column name in Alchemer export¥ LevelPattern: Regex pattern to extract clean level names¥ DisplayOrder: Order for output display (1, 2, 3...)¥ ChartColor: Hex color for visualization6. File Structure6.1 R Module FilesUpdate the module with these files:1. 00_main.R - Entry point, orchestrates workflow2. 01_config.R - Configuration loading and validation3. 02_validation.R - Data validation and quality checks4. 03_analysis.R - Core analysis (mlogit + clogit)5. 04_output.R - 8-sheet Excel output generation6. 05_alchemer_import.R - NEW: Alchemer data transformer7. 06_simulator.R - NEW: Market simulator generation6.2 DependenciesRequired R packages:¥ mlogit - Primary estimation engine¥ survival - Fallback clogit estimation¥ openxlsx - Excel I/O with formatting¥ haven - SPSS/Stata file support (optional)7. Implementation Phases7.1 Phase 1: Core Upgrade (Priority)Focus on getting mlogit working with Alchemer data:1. Create 05_alchemer_import.R with data transformation logic2. Refactor 03_analysis.R to use mlogit as primary engine3. Test with DE_noodle_conjoint_raw.xlsx to verify results4. Compare utilities/importance against ChatGPT statsmodels results7.2 Phase 2: Output EnhancementBuild the market simulator and enhanced output:5. Create 06_simulator.R for market simulator sheet generation6. Expand 04_output.R to 8-sheet structure7. Implement Excel data validation dropdowns8. Add chart data sheets and formatting7.3 Phase 3: None Option SupportHandle Single Choice with None designs:9. Detect and parse None option in Alchemer exports10. Implement alternative-specific constant estimation11. Include None option in market simulator8. Validation Requirements8.1 Statistical ValidationResults must be validated against known benchmarks:¥ Compare against statsmodels results from ChatGPT analysis (gpt_statsmodel_conjoint_logistic.docx)¥ Verify importance scores are non-zero for all varying attributes¥ Confirm zero-centering produces utilities that sum to zero within each attribute¥ Check McFadden R? falls in reasonable range (0.1-0.4 typical for CBC)8.2 Test DataUse these files for validation:¥ conjoint_test_data.csv - Simple 4-attribute synthetic data¥ DE_noodle_conjoint_raw.xlsx - Real Alchemer export (6 attributes, ~5000 rows)Expected results for DE_noodle data (from statsmodels analysis):¥ NutriScore should be dominant attribute (~50-60% importance)¥ Price importance ~10-20%¥ MSG, PotassiumChloride, I+G, Salt should have NON-ZERO importance¥ All attributes should show meaningful utility variation8.3 Known Issue: Zero ImportanceThe ChatGPT scikit-learn analysis produced 0% importance for MSG, PotassiumChloride, I+G, and Salt. This was caused by using standard logistic regression instead of conditional logit. The mlogit implementation must NOT reproduce this error. If any attribute shows 0% importance, investigate:¥ Data quality (is there variation in that attribute?)¥ Multicollinearity between attributes¥ Model specification issues9. Technical Notes for Implementation9.1 mlogit Data PreparationCritical: mlogit requires specific data structure:¥ Long format: one row per alternative per choice set¥ Choice sets must be uniquely identified across respondents¥ Alternative IDs must be consistent within choice sets¥ Panel structure (repeated choices) handled via id.var9.2 Coefficient Extractionmlogit returns coefficients differently than clogit:coefs <- coef(model)  # Named vectorvcov <- vcov(model)   # Variance-covariance matrixse <- sqrt(diag(vcov))  # Standard errorsCoefficient names follow pattern: AttributeLevel (e.g., 'PriceLow', 'BrandApple')9.3 Excel Simulator FormulasKey formulas for market simulator:Utility lookup: =VLOOKUP(selected_level, utility_table, 2, FALSE)Total utility: =SUMPRODUCT(-- applied to all attribute lookups)Share: =EXP(total_utility)/SUM(EXP(all_utilities))Use named ranges for clarity and maintainability.10. Implementation Guidance for Claude10.1 Capabilities AssessmentClaude Sonnet 4.5 is well-suited for this task because:¥ Strong R programming capabilities with statistical packages¥ Experience with mlogit and discrete choice modeling¥ Excel formula generation and openxlsx formatting¥ Can maintain consistency across multiple file updates10.2 Recommended ApproachFor best results:6. Start with 05_alchemer_import.R - create and test independently7. Refactor 03_analysis.R incrementally - keep clogit as fallback8. Build simulator logic in 06_simulator.R before integrating9. Test each component with DE_noodle data before moving on10. Validate results against the statsmodels benchmarks10.3 Potential ChallengesAreas requiring careful attention:¥ mlogit data preparation is precise - test data structure before modeling¥ Excel formula complexity in simulator - build incrementally¥ Alchemer format variations - handle edge cases in import¥ openxlsx dropdown validation syntax - reference documentation10.4 Success CriteriaThe implementation is complete when:¥ DE_noodle_conjoint_raw.xlsx processes without errors¥ All 6 attributes show non-zero importance values¥ Market simulator produces sensible share predictions¥ Output matches professional quality standards¥ Results are reproducible and documentedAppendix A: Reference DataA.1 DE Noodle Dataset StructureColumns: ResponseID, SetNumber, CardNumber, Price, MSG, PotassiumChloride, I+G, Salt, NutriScore, ScoreAttribute levels:¥ Price: Low_071, Mid_089, High_107¥ MSG: MSG_Absent, MSG_Present¥ PotassiumChloride: PotassiumChloride_Absent, PotassiumChloride_Present¥ I+G: I+G_Absent, I+G_Present¥ Salt: Salt_Normal, Salt_Reduced¥ NutriScore: A, B, C, D, ETotal rows: ~5,176 | Respondents: ~350 | Sets per respondent: 5 | Cards per set: 3A.2 Expected Utility Ranges (from statsmodels)Approximate zero-centered utilities from ChatGPT analysis:¥ Price: Low +0.3 to +0.8, Mid +0.1 to +0.4, High -0.3 to -0.5¥ MSG: Absent +0.2 to +0.3, Present -0.2 to -0.5¥ NutriScore: A +1.2 to +1.6, B +0.7 to +0.9, C -0.1 to -0.2, D -0.7 to -1.0, E -1.2 to -1.7Use these ranges as sanity checks for mlogit output.Ñ End of Specification ÑTuras Conjoint Module - Technical SpecificationPage 1 of 2