Below is a future-proof specification you can reuse across Turas modules. It defines the Refusal Pattern (controlled stop, not a crash) and the Mapping Pattern (exact term?level mapping with a hard validation gate). This is written so a developer can implement it consistently, and you can review future modules against it.Turas Reliability SpecificationRefusal and Mapping PatternPurposeTuras modules must never:* guess silently when uncertainty exists,* degrade silently into partial results,* produce outputs that look valid when key assumptions are violated.Instead, modules must either:1. produce fully validated outputs, or2. refuse to run with a clear, user-actionable message.1) Refusal Pattern1.1 DefinitionA refusal is a controlled, intentional stop triggered by:* config issues,* data issues,* missing required packages,* model constraints (e.g., separation without fallback),* invalid assumptions (e.g., outcome mismatch),* incomplete mapping validation.A refusal is not a programming error. It must not look like a crash.1.2 Required interfaceAll modules must implement a single refusal mechanism:Function signature (conceptual):* turas_refuse(code, message, fix=NULL, details=NULL)Returns a structured condition/exception object.Handler:* with_refusal_handler({ ...module_run... })Catches refusals and prints a clean block:* Title: REFUSED TO RUN (CONFIG/DATA MISMATCH)* Code: stable machine-readable string* Reason: one-line human explanation* Fix: actionable steps* Details: optional diagnostics (safe to show)1.3 When to refuse vs when to errorRefuse when the user (you) can fix it by changing config/data/environment.Examples:* required sheet/column missing* outcome_type mismatch* target_outcome_level missing* missing package required for declared mode* mapping coverage incomplete* rare-level policy set to “error”* separation detected and fallback unavailableThrow a real error (stop) only for internal bugs / impossible states.Examples:* inconsistent internal object structure caused by code* NULL pointer where code guarantees non-NULL* unreachable branch reached1.4 Refusal codes (contract)Refusal codes must be:* stable over time,* unique,* searchable in logs,* documented.Recommended pattern:* CFG_* for config* DATA_* for data validation* MODEL_* for model fit/engine issues* MAPPER_* for mapping integrity* PKG_* for dependency issuesExamples from CatDriver:* CFG_OUTCOME_TYPE_INVALID* DATA_OUTCOME_TOO_FEW_CATEGORIES* PKG_NNET_REQUIRED* MODEL_COEF_EXTRACT_FAILED* MAPPER_UNMAPPED_COEFFICIENTS2) Mapping Pattern (Exact term?level mapping)2.1 DefinitionWhen a statistical model produces coefficients, the module must map those coefficients exactly back to:* predictor (driver/control),* factor level (category),* outcome level (for multinomial),* and reference level.This mapping must be:* deterministic,* exact (no guessing),* validated against the model coefficient set.2.2 Forbidden approachesThe following are explicitly forbidden for production:* substring parsing of coefficient names (e.g., substring(term, nchar(var)+1))* heuristic level extraction using startsWith() + suffix guesses* “warn and proceed” when mapping is incomplete* fallback to a weaker mapper on failure2.3 Required approachMapping must be based on the design matrix created by the model formula:1. Build the model frame and design matrix deterministically:o mf <- model.frame(formula, data=...)o mm <- model.matrix(terms(mf), mf) (or equivalent)2. Use design-matrix attributes to link columns to predictors:o attr(mm, "assign")o terms(...)o factor levels (xlevels) and contrast specs3. Construct a mapping table with these minimum fields:* coef_name (as used by the fitted model)* predictor* level (or NA for continuous/control-only)* reference_level* outcome_level (multinomial only)* is_reference (logical)2.4 Strict mapping validation gate (non-negotiable)After fitting the model, the module must perform a hard validation gate:Inputs:* mapping table* the complete set of coefficient names from the fitted modelRules:* If coefficient extraction fails ? REFUSE (MODEL_COEF_EXTRACT_FAILED)* Remove only explicitly “non-mappable” coefficients:o (Intercept)o ordinal threshold parameters (engine-specific; e.g., containing |)* If mapping is empty while remaining coefficients exist ? REFUSE (MAPPER_EMPTY_MAPPING)* If any remaining coefficients are not in mapping ? REFUSE (MAPPER_UNMAPPED_COEFFICIENTS)* If any design matrix column cannot be exactly matched to a known factor level ? REFUSE(MAPPER_LEVEL_MATCH_FAILED)Outcome:* Only proceed to OR extraction / outputs if mapping validation passes.2.5 Multinomial coefficient extraction standardFor multinomial models, coefficients are commonly structured as a matrix.The module must:* extract the correct coefficient-name set in a model-type-specific way* validate mapping coverage for every outcome-level equation or for the target mode, per config.3) No silent degradation policy3.1 PrincipleIf the module cannot guarantee correctness, it must refuse.3.2 Examples of “silent degradation” that must never occur* mapping incomplete but OR tables still produced for the mapped subset* fallback to a weaker mapper method* separation detected but model proceeds with unstable estimates because fallback isn’t installed* multinomial results produced without specifying which outcome is being described* missingness causes sample shrinkage without explicit reporting4) Test requirements for future modulesEach new module must ship with a minimum regression suite:4.1 Golden dataset test* a real-ish fixture dataset + config + expected directional outcomes* asserts key signs/directions (not exact numeric matches)4.2 Mapping integrity test* messy factor labels (spaces, punctuation)* asserts mapping passes or refuses (never warns and continues)4.3 Refusal behaviour test* intentionally broken config scenario* asserts:o refusal code returnedo refusal message printedo no stack-trace crash output5) Review checklist (what you can use in future)A module is “world class reliable” if:* ? All config/data mismatches ? refusal (not stop)* ? Refusal messages are clear and actionable* ? Mapping is design-matrix based (no parsing)* ? Mapping validation is a hard gate* ? No fallbacks that weaken correctness* ? Missing policy is explicit and reported pre/post* ? Tests cover golden + mapping + refusal behaviourIf you’d like, I can turn this into:* a one-page Turas internal standard (v1.0) with a short “Do/Don’t” section, or* a developer template they must copy into each new module’s repo (including stub tests and a refusal-code registry).