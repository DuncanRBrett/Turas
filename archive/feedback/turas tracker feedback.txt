Short answer: based on the code that’s actually in Archive.zip, the tracker module is generally well-structured and I don’t see any show-stopping bugs or obvious logic disasters. The main risks are (a) environment/folder assumptions, (b) how significance testing is handled under weighting, and (c) a couple of edge-case limitations that you’ve already partially documented (e.g. multi-mention).I’ll go through the key pieces and then call out what I’d treat as “watch carefully in production”.1. Overall architecture & wiringWhat I checked:* run_tracker.R (main entry)* run_tracker_gui.R (Shiny launcher)* tracker_config_loader.R* question_mapper.R* wave_loader.R* trend_calculator.R* banner_trends.R* formatting_utils.R* tracker_output.R* validation_tracker.R* constants.R* test_wave_history.R, test_quick_fixes.R* BUG_FIX_SUMMARY.md, FINAL_FIX_SUMMARY.md, WAVE_HISTORY_WALKTHROUGH.md, USER_MANUAL.mdEntry & sourcing* run_tracker.R:o Loads openxlsx.o Derives script_dir via sys.frame(1)$ofile with a fallback to getwd() and then source()s all the module files from there.o Clear, linear pipeline:1. Load config2. Load question mapping + build index3. Validate config4. Load wave data5. Validate wave data6. (optional) banner segments7. Calculate trends8. Write outputVerdict: The wiring is consistent; I don’t see any missing source()s or obvious circular dependencies.Risk: It relies heavily on being run from a sane directory:* If you source("run_tracker.R") from modules/tracker, you’re fine.* If you call run_tracker() from some arbitrary working directory without that, the getwd() fallback could point at the wrong place and the source() paths will break.If you want this to be “harder to misuse”, I’d consider:* Adding an explicit check early in run_tracker() that all required source files have loaded (e.g. exists("load_tracking_config"), exists("calculate_all_trends") etc.) and fail with a clear message if not.* Or enforcing an explicit TURAS_ROOT env variable and deriving paths from that, rather than relying purely on working directory.2. Config & mappingtracker_config_loader.R* Validates file existence.* Reads Waves, Settings, Banner, TrackedQuestions from tracking_config.xlsx.* Validates required columns and returns a clean config list with config_path stored – this is used later for default output location, which is sensible.load_question_mapping()* Checks file and sheet existence.* Validates required columns: QuestionCode, QuestionText, QuestionType.* Wave columns are detected flexibly:o Filters out known metadata cols (QuestionCode, QuestionText, QuestionType, SourceQuestions, TrackingSpecs).o Treats any remaining column with >50% non-empty cells as a wave column, so supports W1/W2/W3 andWave1/Wave2/....* If no wave columns found ? explicit stop() with a good message.question_mapper.R* Builds a bidirectional map:o StandardCode ? per-wave codes.o Per-wave code ? StandardCode, using actual WaveID values from config$waves$WaveID (so not hard-wired to “Wave1, Wave2…” anymore).Verdict: Config and mapping are robustly handled; the logic is defensive and should fail fast if something is off. No obvious bugs here.3. Wave loading, cleaning & weightingwave_loader.R::load_all_waves()Key behaviours:* Logs what it’s doing and which waves it loads.* Calls extract_categorical_question_codes() to find:o All banner variables (so they are always treated as text).o Categorical tracked questions from the mapping.* Loads each wave file via a helper (load_wave_data()), which:o Handles CSV/Excel formats.o Applies the “categorical vs numeric” treatment.Cleaning logic (clean_wave_data())* For each column:o Skip columns already numeric.o Skip columns in categorical_cols.o Detect candidate numeric columns via:* has_numbers = any value contains digits, OR* is_question_col = column name matches ^Q[0-9]+(_[0-9]+)?$.* For these candidate numeric columns:o Replace non-response text codes (DK, Don't Know, NS, NR, Prefer not to say, Refused, N/A, NA) with NAusing a safe pattern:non_na_idx <- which(!is.na(col_data)); ... compare upper-cased trimmed text – this avoids the classic toupper(x) on NA warnings.o Replace comma decimals with periods.o Convert to numeric with suppressWarnings(as.numeric(...)).o Check if conversion created additional NAs; if yes, it prints a diagnostic.Verdict on cleaning: This is cautious and pretty R-safe. The explicit NA handling around toupper() is good and addresses one of the most common “silent bug / warning flood” problems.Edge-case caveat:* Because the trigger uses has_numbers on values, any numeric ID field (e.g. CustomerID) will also be converted to numeric. That’s usually fine, but you lose leading zeros and any text formatting.* For truly non-question numeric identifiers, this is not logically wrong but is a bit over-eager.If you want to tighten it for “100% safe” use:* Consider dropping the has_numbers condition and only clean columns where:o The column name matches your Q pattern oro The variable appears in the question map.* Or make the non-response cleaning list configurable in the config rather than hard-coded.Weighting (apply_wave_weights() and related)* If a weight variable is specified:o Checks the variable exists; if not, warns and falls back to unweighted (weight_var = 1).o Flags missing weights and zero/negative weights, and sets invalid ones to NA.o Sets a standardized wave_df$weight_var.o Computes a simple effective?N ((?w)^2 / ?w?) and prints it.* If no weight variable is specified:o Creates weight_var = 1 and treats it as unweighted.Verdict: Sensible behaviour; no critical bugs in the weight application itself.4. Validationvalidation_tracker.R* Top-level validate_tracking_config() checks:o Config structure.o Wave definitions consistency.o Question mapping integrity vs waves.o Banner structure.o Tracked questions list.wave_loader.R::validate_wave_data()* Confirms:o Every defined wave in config$waves$WaveID is present in wave_data.o Required columns exist.o Wave IDs match between config/mapping/data.o Mapped questions exist in the data (and warns if some are missing).Verdict: Validation is comprehensive enough that if something basic is wrong, you’ll be told before any analysis runs. That’s exactly what you want in a frequently used module.5. Trend calculation & significanceQuestion types & routingtrend_calculator.R:* Normalises question types using normalize_question_type() to internal “engine” labels:o Single_Response, SingleChoice ? single_choiceo Multi_Mention, MultiChoice ? multi_choiceo Rating, Likert, Index, Numeric ? ratingo NPS ? npso Open_End, OpenEnd ? open_endo Ranking ? rankingo Composite ? composite* Dispatcher:o rating ? calculate_rating_trend_enhanced()o nps ? calculate_nps_trend()o single_choice ? calculate_single_choice_trend_enhanced()o multi_choice / Multi_Mention ? calculate_multi_mention_trend()o composite ? calculate_composite_trend_enhanced()o open_end/ranking ? explicitly skipped with warnings.Verdict: The mapping is clear, and you’ve already wired in enhanced functions that respect TrackingSpecs etc. That’s good.Metric calculation (ratings example)calculate_rating_trend_enhanced():* Reads TrackingSpecs per question; if blank ? defaults to "mean".* For each wave:o Extracts the correct wave-specific column using the question map.o Calls calculate_weighted_mean(values, weights); that function:* Enforces values must be numeric (and fails with a helpful message if not).* Filters to valid values (!is.na(values) & !is.na(weights) & weights > 0).* Returns mean, sd, n_unweighted = length(values), n_weighted = sum(weights).* Stores per-wave results including values and weights so you can compute distributions too.No obvious bugs in the core mean / top-box / etc. routines; the NA handling and types look correct.Significance tests* Means:o perform_significance_tests_means():* Uses config alpha (default DEFAULT_ALPHA = 0.05).* Uses config minimum_base (default DEFAULT_MINIMUM_BASE = 30).* Only tests when both waves are available and n_unweighted >= min_base.* Calls t_test_for_means() which is a standard pooled-variance two-sample t-test:* Pooled variance formula* Uses n1 + n2 - 2 df* Two-sided p-value* Returns t_stat, p_value, significant.* Proportions:o perform_significance_tests_proportions():* Same alpha and min_base logic (using n_unweighted).* Calls z_test_for_proportions():* Pooled proportion p_pooled.* Standard error sqrt(p_pooled*(1-p_pooled)*(1/n1 + 1/n2)).* Protects against se == 0.* Returns z_stat, p_value, significant.Important caveat (not a bug, but a limitation):* All tests use unweighted n (n_unweighted) for sample size, while the means/proportions themselves are weightedusing wave_df$weight_var.That means:* If your weights don’t vary much, this is fine and close to usual practice in MR.* If your weights are very skewed (big design effects), the p-values will generally be too optimistic (i.e., overstating significance).If you want to tighten this statistically, the ideal tweak would be:* Use the effective base (eff_n) you already compute (calculate_weight_efficiency()) instead of n_unweightedin the t- and z-tests. Even that is still an approximation, but it’s much better aligned with the weighted estimation.Also, the t-test is an equal-variances t-test. If you are happy with that assumption (which is pretty standard in MR trackers), it’s fine. If you want to be purist, you could replace it with a Welch t-test (separate variances, different df formula).6. Banners & wave historybanner_trends.R* calculate_trends_with_banners():o Takes banner_segments (from get_banner_segments()).o For each question and each segment:* Subsets wave_data appropriately.* Calls the same trend functions as for Total.* This is exactly where your earlier GUI bug lived (TrackingSpecs respected in Phase 2 but ignored in the banner path). The fix described in BUG_FIX_SUMMARY.md and FINAL_FIX_SUMMARY.md is present here – the code now calls the enhanced functions that understand TrackingSpecs.Wave history* There is explicit test scaffolding in test_wave_history.R and a clear walkthrough in WAVE_HISTORY_WALKTHROUGH.md.* The “Q45 multi-category” bug appears to be correctly patched based on FINAL_FIX_SUMMARY.md.Remaining limitation from your own notes:* Q10 Multi_Mention + certain TrackingSpecs combinations are still noted as problem cases in FINAL_FIX_SUMMARY.md (Q10_ISSUES). That’s not “hidden”; you’ve already documented it.I would treat multi-mention + complex specs as:* Supported for common/simple cases.* “Known rough edge” where you should sanity-check outputs until those Q10 issues are explicitly fixed/tested.7. Output & formattingformatting_utils.R + tracker_output.R* Both implement find_turas_root() to locate shared/formatting.R by:o Checking for a global TURAS_ROOT.o Walking up parent directories looking for:* launch_turas.R or* shared/ + modules/.o Finally, trying a small set of relative paths (../.., ../../.., …).* If not found, they stop() with a clear message.Once shared/formatting.R is found:* They import shared formatting helpers like create_excel_number_format() and use that consistently.* write_tracker_output():o Generates default filename as<SanitizedProjectName>_Tracker_<YYYYMMDD>.xlsx in output_dir (config or config’s folder).o Creates a single openxlsx workbook and writes:* Summary sheet (project info, wave table, basic stats).* Detailed sheets (with/without banner breakouts).* Optional wave history sheet, depending on report_types.Verdict: Output handling is coherent and leverages your shared formatting; no obvious logical faults.Environment warning:* This module is not standalone. It requires the broader Turas structure (shared/ folder). If someone plucks just the tracker folder and tries to use it independently, find_turas_root() will deliberately blow up.For your own workflow that’s okay, but I’d document that very clearly in the developer manual.8. GUI wrapper (run_tracker_gui.R)* Checks for shiny and shinyFiles, installs missing packages if needed.* Manages a simple “recent projects” list via RDS in a data/ folder beside the script.* Uses runApp() with a full UI definition (file chooser, use banners toggle, recent project selection, etc.).No critical issues jumped out – it’s a straightforward Shiny launcher. The main risk is again path management (making sure it’s run from the right folder so it can see modules and shared).9. My practical recommendationsNothing I’ve seen looks like a latent “this will randomly crash or silently corrupt numbers” bug. But if you want it as close to bulletproof as possible, I’d prioritise these refinements:1. Tighten environment assumptionso Add a small helper, called right at the top of run_tracker(), that:* Verifies required functions exist (load_tracking_config, calculate_all_trends, etc.).* Verifies shared/formatting.R is findable via find_turas_root().* If not, stops with a one-line “How to fix” message.o Encourage setting a global TURAS_ROOT in launch_turas.R so the tracker doesn’t have to guess.2. Improve significance under weightingo Replace n_unweighted with an effective N in the t- and z-tests:* You already have calculate_weight_efficiency(weights); expose it in wave_results as eff_n and use it.o Optionally switch t_test_for_means() to a Welch t-test; but that’s a design choice, not a must-fix.3. Narrow the cleaning scope slightlyo In clean_wave_data(), consider:* Only applying the non-response ? NA + numeric conversion to:* Columns whose names look like question codes (Q…) or* Columns explicitly referenced in the question mapping.o This reduces the chance of “accidentally helpful” behaviour on odd columns.4. Explicitly flag known limitations in manualso You already do this partially, but I’d make it very explicit in USER_MANUAL.md:* Multi-mention + complex TrackingSpecs are still “experimental” (until you’ve fully ironed out Q10).* Open-ends and rankings are intentionally not tracked.* Significance tests are approximate under strong weighting (and you may later upgrade them).Bottom lineFrom what’s in Archive.zip, this tracker module is in good production shape:* Strong validation.* Sensible cleaning.* Robust handling of config/mapping.* Clear trend & significance logic.* Integrated bug-fix history and tests.I don’t see any “red-flag” errors that would make me afraid to rely on it, provided:* It’s run inside the proper Turas folder structure.* You’re aware of the weighting/significance approximation.* You treat multi-mention + fancy TrackingSpecs as an area to keep an eye on.If you’d like, next step I can:* Propose concrete patched snippets specifically for the significance tests (switching to effective N), and/or* Draft a short “developer note” section summarising the environment assumptions and limitations to drop straight into your internal docs.Short version: it’s already very capable, but there are a handful of things that come up again and again in real-world trackers that aren’t fully “baked in” yet. Most of them are around questionnaire changes, diagnostics, and “story-ready” output, rather than the core maths.I’ll group them and be blunt about what I’d call must-have vs nice-to-have for a working MR agency.1. Handling questionnaire / scale changes (this bites everyone)Must-have1. Scale change & recoding supporto Common scenarios:* 5-point scale ? 7-point scale.* Wording change mid-tracker (“Very satisfied” ? “Completely satisfied”).* NPS asked in one wave, dropped or asked differently in another.o Features you’d want:* A “bridging” layer where you can define:* “Old 5-point scale codes 4–5 map to new 7-point codes 6–7” for trend continuity.* Or force a break in series and start a “v2” trend.* A way to tag a break in the time-series and clearly show:Trend (old scale) up to 2025Q2; Trend (new scale) from 2025Q3 – not strictly comparable.2. Question renaming / splitting / mergingo E.g. Q10 split into Q10a/Q10b; or two items combined into one.o You partly handle this with the question mapping, but in real life you often need:* A “Derived tracking question” that pulls from different raw variables in different waves using recode logic (not just mapping to a single column name).* A flag in the config to mark “trend is partly derived / reconstructed” so you don’t forget.2. Trend mechanics: how clients actually want to see itMust-have3. Rolling averages / smoothingo For high-noise trackers (especially with smaller n per wave), clients love:* 3-wave rolling averages.* 3-month / 6-month rolling windows on continuous field trackers.o The system currently does “point per wave”; I’d add:* Optional rolling_mean and rolling_top2box on key KPIs.* Config-level flag: TrendSmoothing = "None" | "3wave" | "custom".4. Flexible comparison logico Right now, the default is essentially “latest vs previous” (and sometimes vs base).o In practice you’ll want:* CompareTo = "Previous" | "SamePeriodLastYear" | "CustomWaveID".* The ability to define multiple comparisons for a KPI, e.g.:* Current vs previous wave.* Current vs same wave last year (to handle seasonality).Nice-to-have5. Seasonality handlingo Even just the ability to flag certain waves as “promo months” or “low season” and:* Store those flags.* Use them in visual outputs (e.g., shaded background).o This doesn’t need fancy time-series modelling – just metadata and visual cues.3. Diagnostics: is this change real or just noise?You already have a separate “confidence/validity” module, which is great. But in trackers, some of this needs to be surfaced inside the trend outputs.Must-have6. Sample-mix diagnostics per waveo For each wave (and key segments):* Composition vs target (or vs a reference wave) on Age, Gender, Region, Channel, etc.* A simple drift flag:* “Large sample mix shift vs Wave 3 on Region and Channel.”o Helps to interpret weird KPI jumps (“NPS up 8 points, but the wave is 20% more ‘large customers’ than usual”).7. Design effect / effective-N in the trend outputso You already calculate something like effective N for weights, but it’s not fully exploited in the tracker.o I’d want:* For every trend line: n_unweighted, n_effective, DEFF.* Optionally display a “low precision” warning when effective n drops below a threshold.8. Multiple-comparison control (at least reporting)o You’re testing lots of metrics across many segments/waves.o Bare minimum:* A simple indication in the output that p-values are unadjusted, or* A configurable option for:* Bonferroni (too strict but simple), or* Holm-Bonferroni.o Or, at least, a per-sheet note: “Significance testing is unadjusted and should be interpreted as directional”.Nice-to-have9. “Exceptions” or alerts listo One of the most useful real-world features:* A single sheet that summarizes:* All metrics / segments where change exceeds X points and is significant.* Sorted by effect size or KPI importance.o This is effectively your “What changed this wave?” one-pager.10. Structural break detection (light-touch)o Even just:* A simple rule: if we get 3 consecutive significant moves in the same direction, mark that as a “structural shift”.o It gives you a flag: “This seems like a level shift, not random noise”.4. Question types & metrics clients keep asking forThe engine handles rating, single choice, NPS, composite, multi-mention (with caveats). There are a few common asks you might want to bake in further:Must-have11. Standardized “Top2Box / Bottom2Box / Net” for ratings* You already handle means and some specs via TrackingSpecs, but in practice:o Clients constantly want mean + Top2Box (%) + Net Top2–Bottom2 as standard.* I’d:o Define profile types like RatingProfile = "MeanOnly" | "Mean+Top2" | "Top2Only".o Or standardize a default rating pack:* Mean, SD, Top2Box, Bottom2Box, Net.12. Net metrics for multi-mention and brand codes* E.g. Brand health trackers:o “Any positive attribute”, “Any negative attribute”, “Net positive–negative”.* At configuration level, allow:o “Net metrics” defined as differences or unions of multiple codes and treat them as tracked metrics.Nice-to-have13. Support for simple indices inside tracker* You do have a separate module for indices / KDA, but sometimes you want:o Simple additive/averaged indices directly inside the tracker, e.g. “Service Index = mean(Q1, Q2, Q3)”.* That could be defined in TrackedQuestions as:o QuestionType = "Composite" and SourceQuestions = "Q1,Q2,Q3" with a simple rule.14. Light support for ranking questions* Even just:o Track the % ranked 1st for each item over time.o Track mean rank per item.* This wouldn’t require full ranking analytics — just a basic “Time trend on ranked 1st”.5. Output & workflow niceties (time savers)Must-have15. Standard “tracker pack” template exports* Right now you output Excel; that’s vital.* But in practice, you often want:o A fixed set of CSVs (one per “view”) for dashboards.o Or, eventually, PowerPoint templates you can populate (but that might live in another module).* At minimum, I’d add:o A config flag for “dashboard export”:* Long-format tables ready for BI (Wave, Segment, MetricName, Value, n, etc.).16. Strong versioning / audit trail in the output* You already put project name and date in the filename.* I’d also include in the Summary sheet:o The Git commit or code version of the tracker module (manually entered or env var).o The path to the config and mapping files used.o Date/time run, user name (if easy to capture).* It saves you when a client asks “Why are the numbers different to last week’s run?”Nice-to-have17. Event markers / annotations* Tracker reality: “We ran a promotion here; we changed call centre here; load shedding spiked here.”* Add:o A simple Events sheet in the config with WaveID ? EventLabel.o Carry those labels through so downstream graphing (in Excel/PowerPoint/R) can annotate them.18. “KPI catalogue” sheet* For each tracked question / metric:o Human-readable description, question wording, scale, and any caveats.* You’ve got the ingredients in config + mapping; it’s a small step to output a polished “KPI dictionary” tab.6. Multi-country / multi-client scaling (if you go that way)If you later use the same module across multiple countries or business units, some things become important:Nice-to-have19. Hierarchical banners* Banners that can be:o Country ? Region ? Segment, not just a flat segment list.* Makes it easier to run one tracker across markets while still getting local views.20. Country-level weights / targets* If you ever use it in multi-country trackers, you may want:o Country-specific targets and weights.o A quick cross-check that global KPIs are properly aggregated (weighted by country size, not just equal wave n).How I’d prioritise changes for youIf we were hard-nosed and aiming for “what will help Duncan most with clients in the next 6–12 months”, I’d put it like this:Top priority (I’d treat these as backlog items 1–5):1. Scale / questionnaire change handling (bridging or explicit trend breaks).2. Rolling averages / smoothing for key KPIs.3. Effective-N and simple drift diagnostics (composition vs target / vs reference wave).4. An “exceptions / alerts” sheet summarising big, significant moves.5. Standardised rating pack (Mean + Top2Box + Net) and net metrics.Second priority:6. Event markers (promo, outages, org changes).7. Dashboard-friendly long-format exports.8. Simple multiple-comparison note or optional adjustment.9. Basic ranking support (Top 1st rank over time).10. Stronger environment/version info in outputs.If you like, next step I can help you turn this into:* A concrete dev brief for the tracker module (like we’ve done for the other modules), with:o Feature list,o Data structures,o Example configs,o And how they’ll appear in the Excel outputs.