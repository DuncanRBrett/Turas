Developer task list: make CatDriver world-class reliable (internal use, messy data, no silent failures)0) Define “done” and lock the contract0.1 Create a short “CatDriver Spec v1.0” doc* Outcome types supported: binary / ordinal / multinomial (explicit).* Driver types supported: categorical / ordinal / continuous (explicit).* Missingness strategies supported.* Weight handling stance (and what p-values/CI mean).* Default behaviours + override rules.* Output workbook schema (sheet names + required columns).Acceptance criteria* Any behaviour not explicitly defined above must produce a hard error (“Refuse to run”) rather than guessing.1) Eliminate term?level mapping fragility (highest priority)Your current substring/name-parsing approach will break on real-world labels.1.1 Implement a canonical “design-matrix mapper”Create a utility: map_terms_to_levels(model, data, terms_obj) that returns a mapping table like:* driver, level, design_col, coef_name, reference_level, is_reference, outcome_level (if multinomial)Implementation approach* Generate model matrix using the exact formula/terms used in fitting:o mm <- model.matrix(terms(model), model.frame(model)) (or equivalent for ordinal/multinom)* Use:o attr(mm, "assign") + terms(model) to map columns ? predictorso xlevels / contrasts to map columns ? factor levels* Never infer levels by string slicing.Acceptance criteria* Labels with spaces, punctuation, leading numbers, etc. map correctly.* Mapping is identical across macOS/Windows and across R versions (within reason).2) Fix outcome ordering + interpretation (trip-wire)2.1 Enforce outcome order from config* Before model fitting, coerce outcome:o ordinal: ordered(outcome, levels=config_levels)o multinomial: factor(outcome, levels=config_levels) (order still matters for reference choice)* Validate:o all observed outcome values exist in configo config contains at least 2 levels2.2 Add “direction sanity” trip-wire* Compute raw proportions by key drivers (at least for top drivers) and compare to sign of coefficient for a chosen baseline.* If outcome is ordinal and config says Low < ... < High, then “worse groups” should not systematically yield OR>1 with “more satisfied” interpretation.* If mismatch is detected: STOP with a clear error and guidance (“Outcome ordering likely reversed or interpretation flipped”).Acceptance criteria* Your golden dataset: campus=Online must yield OR < 1 when outcome order is Low<Neutral<High.* If config order is reversed intentionally, tool must reflect that in text (“higher category means …”).3) Make driver type handling explicit: categorical vs ordinal vs continuousYou need flexibility without ambiguity.3.1 Add driver typing rulesFor each driver, support one of:* categorical* ordinal* continuousConfig support* Add a DRIVER_SETTINGS sheet (or extend existing) with columns:o driver, type, levels_order (optional), reference_level (optional), missing_strategy (optional), rare_level_strategy (optional)3.2 Remove “numeric with <=10 uniques => categorical” as a hidden rule* Replace with:o default inference rule (still allowed), but must be reported and overrideable per driver.* Output a “Driver typing summary” table in Diagnostics:o driver, inferred_type, final_type, reason.3.3 Ensure contrasts match driver type* categorical: treatment contrasts (one reference)* ordinal predictor: monotonic / ordered effecto choose: (a) treat as numeric scores, or (b) ordered factor with appropriate contrastso whichever you choose, reporting must match (see below)* continuous: keep as numeric (but you said you have a separate continuous KDA module; in CatDriver, continuous should likely be allowed only as covariates/control variables, or disabled)Acceptance criteria* No .L/.Q/.C polynomial terms appear in OR tables unless the driver is explicitly ordinal-with-polynomial mode (and then OR-by-level must be disabled or relabeled).4) Fix multinomial reporting (no more “first match wins”)4.1 Require a multinomial reporting modeChoose one (configurable):* Per outcome level reporting (recommended default)o ORs, p-values, importances for each outcome vs reference outcome* Target outcome onlyo config specifies target_outcome_levelo all interpretation restricted to that target4.2 Fix LR df calculation* Use attr(logLik(model), "df") delta for LR tests consistently.4.3 Update factor patterns for multinomial* If “per outcome” mode:o patterns sheet becomes stacked by outcome level (or separate sheets per outcome)* If “target outcome only”:o patterns sheet uses the target outcome’s coefficient set onlyAcceptance criteria* No multinomial pattern table can be produced without specifying which outcome’s ORs it refers to.5) Missing data strategy: explicit, flexible, and clearly reportedMessy data is normal; silent handling is not acceptable.5.1 Implement per-variable missing strategiesSupported strategies (minimum):* drop_row (complete-case with respect to model vars)* missing_as_level (for categorical/ordinal drivers)* impute_median / impute_mode (for continuous covariates, if allowed)* error_if_missing (strict mode)5.2 Ensure reporting reflects pre and post handlingDiagnostics must include:* missing counts before any transformations* what was done per variable* resulting N used in model* which rows were dropped (count + reason summary)5.3 Prevent missing from becoming the referenceIf missing_as_level, enforce:* reference must be non-missing unless explicitly permitted* if missing is chosen as reference by default rules: STOP with error and guidance.Acceptance criteria* The tool never reports “0% missing” simply because missing was converted to a level; it must show original missingness.6) Rare levels & sparse cells: choose strategy, enforce it6.1 Add “rare level policy”Configurable per driver:* warn_only* collapse_to_other (below threshold N or %)* drop_level (and drop affected rows)* error (refuse to run)Set defaults:* warn if any expected cell count < 5* collapse if any level N < 10 (configurable)6.2 Implement deterministic collapsing* “Other” label must be stable and clearly recorded* mapping tables must reflect collapsing (so ORs refer to final levels)Acceptance criteria* Any collapse/drop must be documented in output and in log.7) Separation and convergence: automatic safe fallbackThis is the difference between “works sometimes” and “reliable for years”.7.1 Add robust fit wrappersFor each model family:* binary logistic:o try glmo if separation/non-convergence/huge SEs: fallback to bias-reduced logistic (e.g., brglm2) or Firth (logistf)* ordinal logistic:o pick a primary engine with good diagnostics (often ordinal::clm is more robust than polr)o define fallback behaviour if non-convergent (e.g., switch optimizer, reduce model, or error with guidance)* multinomial:o detect non-convergence and high standard errorso fallback: penalized multinomial (if you’re willing) or reduced model strategy7.2 Add explicit “fit status” outputs* model engine used* convergence status* fallback used (yes/no + reason)* any predictors dropped due to singularitiesAcceptance criteria* Never silently proceed after a failed fit.* If fallback is used, workbook must state it prominently.8) Importance metrics: make them consistent and interpretable8.1 Standardize importance method per outcome* binary/ordinal: likelihood ratio drop-in-deviance (preferred) OR Type II/III tests, but be consistent* multinomial: LR per predictor per outcome (or overall)8.2 Add a “stability” flag* If model used fallback estimation or heavy collapsing, importance should be flagged as “use with caution”.Acceptance criteria* “Importance” must always clearly state the test type and what it compares.9) Replace OR-only interpretation with probability-lift (recommended)This makes sanity-checking easier and reduces misinterpretation risk.9.1 Add predicted probability summariesFor top K drivers:* ordinal:o report change in P(High) (and optionally P(Low)) between ref level and each level* multinomial:o report change in P(target outcome) or per outcome in per-outcome mode* binary:o report change in P(1) (or success category)9.2 Define baseline profile* either use:o “reference levels for all categorical + median for continuous”, oro allow config to set a baseline respondent profileAcceptance criteria* Probability lift must align with OR directionality in the golden dataset.10) Output workbook: prevent structural corruption and enforce schema10.1 Fix Excel writing pipeline* Ensure the workbook writer doesn’t create relationships to missing drawing parts.* Avoid legacy drawing/VML unless necessary.* Add a “workbook integrity check” step in tests (openpyxl read/write roundtrip, or Excel open validation if you can).10.2 Schema validation* Implement: validate_output_schema(wb) ensuring required sheets/columns exist.Acceptance criteria* Produced XLSX opens cleanly in Excel and can be read programmatically without missing-part errors.Regression tests (must-have)Create tests/testthat/ with golden fixtures stored in inst/extdata/ or similar.T1) Golden ordinal dataset (the one you provided)Dataset: employment_satisfaction.csvExpected: use the golden XLSX/CSV reference (we already created a candidate)Assertions:* Online OR < 1 (given Low<Neutral<High)* Grade A OR >> 1 vs D* No missing becomes reference* Term?level mapping covers all reported levels* Probability lift direction matches OR directionT2) Binary dataset with separationCreate a small synthetic dataset where one category perfectly predicts outcome in a subset.Assertions:* tool detects separation and either:o uses fallback estimator, oro errors with explicit “separation” message* no infinite ORs in final output without a “fallback used” flagT3) Multinomial datasetCreate or source a dataset with 3+ outcomes and at least 2 strong predictors.Assertions:* tool refuses to create single “OR” statements without either:o per-outcome tables, oro configured target_outcome_level* LR df correct and consistent* patterns output tied to an outcome levelT4) Messy labels mapping testDataset where factor levels include:* spaces, punctuation, “1 - Strongly agree”, “(Not sure)”Assertions:* mapping table correct* OR extraction correct* no NA ORs due to parsingT5) Missingness policy testDataset with missing in:* outcome (should error or drop based on policy)* categorical driver* continuous covariateAssertions:* missing pre/post correctly reported* policy applied exactly* “0% missing” never reported when it’s just recodedT6) Rare level collapsing testDataset with rare levels below threshold.Assertions:* collapse happens deterministically* “Other” level reported* mapping and OR tables reflect collapsed levels“No silent failures” enforcement (global)Implement a standard TurasGuard layer:G1) Hard errors (refuse to run)* outcome levels not in config* multinomial with no target and no per-outcome mode* mapping failure (any driver level reported cannot be mapped)* missing becomes reference (unless explicitly allowed)* model fails and no fallback availableG2) Soft failures (warnings in output + log + exec summary)* fallback estimator used* heavy collapsing performed* effective N too small* events-per-parameter too low* proportional odds assumption questionable (ordinal)in addition please note the following clariyfing pointsCatDriver – Plain-English Configuration & Behaviour Guide(Internal use – explicit, safe, no silent failures)1. How weighting is handledWhat this tool does* Survey weights are treated as analysis weights.* Weighted estimates (odds ratios, predicted probabilities) reflect the weighted data.* Statistical tests and confidence intervals are model-based, not survey-design-based.What this means* The tool does not assume information about clustering or stratification.* P-values indicate strength of association, not exact population inference.How this is reported* The output clearly states:“Models are weighted using analysis weights; standard errors are model-based.”2. How missing data is handled (explicit and visible)Outcome variable (what we are explaining)* Rows with missing outcome values are excluded from the model.* This is required because an unknown outcome cannot be modelled.Categorical and ordinal drivers* Missing values are treated as their own category, labelled:“Missing / Not answered”* This allows the analysis to:o keep the sample size stableo show whether missingness itself is informativeWhat is always reportedThe Diagnostics output includes:* missing counts before handling* missing counts after handling* number of rows dropped due to missing outcomes* confirmation that missing was treated as a level (where applicable)The tool will never hide missing data by silently recoding it.3. Supported outcome types (must be declared)CatDriver supports three outcome types, and the config must explicitly declare which one is used.Supported types* Binary (e.g. Yes / No)* Ordinal (e.g. Low / Neutral / High)* Multinomial (e.g. Reason A / Reason B / Reason C)Enforcement rules* The tool checks the data against the declared type.* If the data does not match the declaration:o the analysis refuses to runo a clear message explains why, e.g.:“Outcome declared as ordinal but data contains unordered categories.”This is treated as a controlled stop, not a software error.4. Reference categories (what everything is compared to)Odds ratios are always relative to a reference category.How the reference is chosenIn order of priority:1. Explicitly specified in the config (recommended)2. Most frequent non-missing category3. First category in the declared category orderSafety rules* The reference category is never allowed to be Missing, unless explicitly permitted.* If a valid reference cannot be determined, the tool refuses to run and explains why.5. Rare categories and sparse dataSmall categories can produce unstable or misleading results.How this is handledFor each driver, the config can specify:* a warning threshold (e.g. N < 5)* an auto-collapse threshold (e.g. N < 10)Behaviour* Categories below the warning threshold trigger a warning.* Categories below the collapse threshold are merged into an “Other” category (if enabled).* All collapsing is:o deterministico clearly documented in the outputNothing is collapsed silently.6. Statistical engines used (explicit and transparent)Default engines* Binary outcomes: logistic regression* Ordinal outcomes: a standard, widely used ordinal logistic engine* Multinomial outcomes: multinomial logistic regressionRobustness rules* If the primary model fails to converge or shows separation:o a safer fallback method is used (where available), oro the tool stops with a clear explanationReportingThe output explicitly states:* which model engine was used* whether a fallback was required* whether any predictors were dropped or adjusted7. Multinomial outcomes: no ambiguityMultinomial outcomes do not have a single odds ratio per driver.Required configurationThe config must specify one of:* a target outcome category (e.g. “Price”)* or request per-outcome reportingOutput behaviour* All tables clearly label which outcome category they refer to.* The tool never “picks one” silently.If no valid reporting mode is specified, the tool refuses to run and explains why.8. Driver types (categorical vs ordinal vs continuous)Supported driver typesEach driver must be declared (or explicitly inferred and reported as such):* Categorical* Ordinal* Continuous (allowed only if explicitly supported in this module)Enforcement* The chosen statistical treatment must match the declared driver type.* If there is a mismatch, the tool stops and explains the issue.TransparencyA diagnostics table shows:* inferred type* final type used* reason for the decision9. No silent failures – everCatDriver is designed to stop rather than guess.The tool will refuse to run if:* outcome type does not match data* outcome ordering is ambiguous or inconsistent* multinomial reporting is underspecified* reference category is invalid* term-to-level mapping fails* model fails and no safe fallback existsEach refusal includes:* a clear message* the specific reason* guidance on how to fix it