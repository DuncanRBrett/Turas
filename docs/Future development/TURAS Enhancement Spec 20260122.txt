TURAS Enhancement Spec: Parquet Caching + Golden Dataset TestingVersion: 1.0Purpose: Increase reliability and confidence without disrupting existing workflowsPrinciple: Additive changes only - nothing existing breaks1. Parquet Caching System1.1 GoalReplace CSV caching with Parquet format to:Preserve column types (factors stay factors, dates stay dates)Improve load performance on large datasetsReduce cache file sizesEliminate "character coercion" bugs1.2 Design PrinciplesOpt-in initially - Existing CSV caching continues to workAutomatic migration - Old CSV caches still readableSchema verification - Validate types on loadZero breaking changes - All existing code paths unchanged1.3 ImplementationNew Filesmodules/shared/data/??? parquet_cache.R      # New: Parquet read/write functions??? schema_manifest.R    # New: Schema capture and verification??? data_loader.R        # Existing: Modified to support parquet optionCore Functionsparquet_cache.R#' Write data to parquet with schema manifest#' @param data Data frame to cache#' @param cache_path Path for .parquet file (without extension)#' @return List with status, paths writtenwrite_parquet_cache <- function(data, cache_path) {  # Requires: arrow package    parquet_path <- paste0(cache_path, ".parquet")  schema_path <- paste0(cache_path, "_schema.rds")    # Capture schema before writing  schema <- capture_schema(data)    # Write parquet  arrow::write_parquet(data, parquet_path)    # Write schema manifest  saveRDS(schema, schema_path)    return(list(    status = "PASS",    parquet_path = parquet_path,    schema_path = schema_path,    rows = nrow(data),    cols = ncol(data)  ))}#' Read parquet cache and restore schema#' @param cache_path Path for .parquet file (without extension)#' @param verify_schema If TRUE, warn on schema mismatches#' @return Data frame with restored typesread_parquet_cache <- function(cache_path, verify_schema = TRUE) {    parquet_path <- paste0(cache_path, ".parquet")  schema_path <- paste0(cache_path, "_schema.rds")    if (!file.exists(parquet_path)) {    return(list(      status = "REFUSED",      code = "CACHE_NOT_FOUND",      message = sprintf("Parquet cache not found: %s", parquet_path)    ))  }    # Read parquet  data <- arrow::read_parquet(parquet_path)    # Restore schema if manifest exists  if (file.exists(schema_path)) {    schema <- readRDS(schema_path)    data <- restore_schema(data, schema, warn_mismatches = verify_schema)  }    return(list(    status = "PASS",    data = data,    schema_restored = file.exists(schema_path)  ))}schema_manifest.R#' Capture schema from data frame#' @param data Data frame#' @return List containing column classes, factor levels, attributescapture_schema <- function(data) {  schema <- list(    captured_at = Sys.time(),    turas_version = get_turas_version(),    columns = lapply(names(data), function(col) {      list(        name = col,        class = class(data[[col]]),        levels = if (is.factor(data[[col]])) levels(data[[col]]) else NULL,        ordered = if (is.factor(data[[col]])) is.ordered(data[[col]]) else NULL      )    })  )  names(schema$columns) <- names(data)  return(schema)}#' Restore schema to data frame#' @param data Data frame (possibly with wrong types)#' @param schema Schema manifest from capture_schema#' @param warn_mismatches If TRUE, warn when restoration fails#' @return Data frame with restored typesrestore_schema <- function(data, schema, warn_mismatches = TRUE) {    warnings <- list()    for (col_name in names(schema$columns)) {    if (!col_name %in% names(data)) {      warnings[[col_name]] <- "Column missing from data"      next    }        col_schema <- schema$columns[[col_name]]        # Restore factors    if ("factor" %in% col_schema$class) {      if (!is.null(col_schema$levels)) {        if (col_schema$ordered) {          data[[col_name]] <- factor(data[[col_name]],                                       levels = col_schema$levels,                                       ordered = TRUE)        } else {          data[[col_name]] <- factor(data[[col_name]],                                       levels = col_schema$levels)        }      }    }        # Restore dates    if ("Date" %in% col_schema$class && !inherits(data[[col_name]], "Date")) {      data[[col_name]] <- as.Date(data[[col_name]])    }        # Restore integers    if ("integer" %in% col_schema$class && !is.integer(data[[col_name]])) {      data[[col_name]] <- as.integer(data[[col_name]])    }  }    if (warn_mismatches && length(warnings) > 0) {    message("Schema restoration warnings:")    for (w in names(warnings)) {      message(sprintf("  %s: %s", w, warnings[[w]]))    }  }    attr(data, "schema_restored") <- TRUE  attr(data, "schema_warnings") <- warnings    return(data)}Modification to data_loader.R# Add to existing load_data function or create wrapperload_data_cached <- function(data_path,                               cache_dir = NULL,                               cache_format = "parquet",  # "parquet" | "csv" | "none"                              force_reload = FALSE) {    # Determine cache path  if (is.null(cache_dir)) {    cache_dir <- dirname(data_path)  }  cache_name <- tools::file_path_sans_ext(basename(data_path))  cache_path <- file.path(cache_dir, paste0(".cache_", cache_name))    # Check for existing cache  parquet_exists <- file.exists(paste0(cache_path, ".parquet"))  csv_exists <- file.exists(paste0(cache_path, ".csv"))    # Use cache if available and not forcing reload  if (!force_reload) {    if (cache_format == "parquet" && parquet_exists) {      result <- read_parquet_cache(cache_path)      if (result$status == "PASS") {        message("Loaded from parquet cache")        return(result$data)      }    }        # Fallback to CSV cache if parquet not available    if (csv_exists) {      message("Loaded from CSV cache (consider upgrading to parquet)")      return(data.table::fread(paste0(cache_path, ".csv")))    }  }    # Load from source  data <- load_data_from_source(data_path)  # Existing function    # Write cache  if (cache_format == "parquet") {    write_parquet_cache(data, cache_path)    message("Cached to parquet format")  } else if (cache_format == "csv") {    data.table::fwrite(data, paste0(cache_path, ".csv"))    message("Cached to CSV format")  }    return(data)}1.4 Dependencies# Add to renvrenv::install("arrow")Arrow package is ~30MB but well-maintained and widely used.1.5 Migration PathAdd new files (no existing code touched)Test with one module (e.g., tabs)Gradually update modules to use load_data_cached(..., cache_format = "parquet")Old CSV caches continue to work indefinitely2. Golden Dataset Testing System2.1 GoalCreate "known-good" reference outputs for each module so that:You can verify Turas still produces correct resultsChanges to one module don't silently break othersYou have confidence before delivering to clients2.2 Design PrinciplesSeparate from production code - Tests don't affect module executionSimple to add new golden datasets - Low friction to expand coverageClear pass/fail output - Know immediately if something brokeData content vs formatting - Distinguish statistical changes from cosmetic changes2.3 Structuretests/??? golden/?   ??? README.md                    # How to add/update golden tests?   ??? run_all_golden_tests.R       # Master test runner?   ??   ??? tabs/?   ?   ??? project_001/?   ?   ?   ??? input_data.parquet   # Input data?   ?   ?   ??? input_config.xlsx    # Config used?   ?   ?   ??? golden_output.rds    # Expected output (data only)?   ?   ?   ??? metadata.json        # When captured, Turas version, notes?   ?   ??? project_002/?   ?       ??? ...?   ??   ??? weighting/?   ?   ??? project_001/?   ?       ??? ...?   ??   ??? maxdiff/?   ?   ??? ...?   ??   ??? keydriver/?       ??? ...???? testthat/                        # Existing test infrastructure    ??? ...2.4 Core Functionstests/golden/golden_test_utils.R#' Capture a golden dataset from current output#' Run this when you've verified output is correct#' #' @param module Module name (e.g., "tabs", "weighting")#' @param project_name Identifier for this test case#' @param input_data The input data frame#' @param input_config Path to config file used#' @param output The output from running the module#' @param notes Optional notes about this test casecapture_golden <- function(module,                            project_name,                            input_data,                            input_config,                           output,                           notes = NULL) {    # Create directory  golden_dir <- file.path("tests", "golden", module, project_name)  if (!dir.exists(golden_dir)) {    dir.create(golden_dir, recursive = TRUE)  }    # Save input data  write_parquet_cache(input_data, file.path(golden_dir, "input_data"))    # Copy config  file.copy(input_config, file.path(golden_dir, "input_config.xlsx"),             overwrite = TRUE)    # Extract and save data content only (not Excel formatting)  golden_data <- extract_data_content(output)  saveRDS(golden_data, file.path(golden_dir, "golden_output.rds"))    # Save metadata  metadata <- list(    captured_at = Sys.time(),    turas_version = get_turas_version(),    r_version = R.version.string,    notes = notes,    input_rows = nrow(input_data),    input_cols = ncol(input_data)  )  jsonlite::write_json(metadata, file.path(golden_dir, "metadata.json"),                        pretty = TRUE, auto_unbox = TRUE)    message(sprintf("Golden dataset captured: %s/%s", module, project_name))  return(invisible(golden_dir))}#' Extract data content from module output#' Strips formatting, keeps only statistical content#' #' @param output Module output (could be list, Excel path, etc.)#' @return List of data frames representing the statistical contentextract_data_content <- function(output) {    # If output is a path to Excel file, read all sheets  if (is.character(output) && grepl("\\.xlsx$", output)) {    sheets <- openxlsx::getSheetNames(output)    content <- lapply(sheets, function(s) {      openxlsx::read.xlsx(output, sheet = s)    })    names(content) <- sheets    return(content)  }    # If output is a list with result component  if (is.list(output) && "result" %in% names(output)) {    return(output$result)  }    # Otherwise return as-is  return(output)}#' Compare current output to golden dataset#' #' @param module Module name#' @param project_name Test case identifier#' @param current_output Output from current run#' @param tolerance Numeric tolerance for comparisons#' @return List with status, differences (if any)compare_to_golden <- function(module,                                project_name,                                current_output,                                tolerance = 1e-6) {    golden_dir <- file.path("tests", "golden", module, project_name)  golden_path <- file.path(golden_dir, "golden_output.rds")    if (!file.exists(golden_path)) {    return(list(      status = "REFUSED",      code = "GOLDEN_NOT_FOUND",      message = sprintf("No golden dataset found: %s/%s", module, project_name)    ))  }    # Load golden  golden_data <- readRDS(golden_path)    # Extract current  current_data <- extract_data_content(current_output)    # Compare using waldo for detailed diffs  differences <- waldo::compare(    golden_data,     current_data,    tolerance = tolerance,    max_diffs = 20  )    if (length(differences) == 0) {    return(list(      status = "PASS",      message = "Output matches golden dataset"    ))  } else {    return(list(      status = "FAIL",      message = sprintf("%d differences found", length(differences)),      differences = differences    ))  }}#' Run a single golden test#' #' @param module Module name#' @param project_name Test case identifier#' @return Test resultrun_golden_test <- function(module, project_name) {    golden_dir <- file.path("tests", "golden", module, project_name)    # Load input  input_result <- read_parquet_cache(file.path(golden_dir, "input_data"))  if (input_result$status != "PASS") {    return(input_result)  }  input_data <- input_result$data    config_path <- file.path(golden_dir, "input_config.xlsx")    # Run module  # This requires a standardized way to run each module  current_output <- run_module(module, input_data, config_path)    # Compare  result <- compare_to_golden(module, project_name, current_output)  result$module <- module  result$project <- project_name    return(result)}tests/golden/run_all_golden_tests.R#' Run all golden tests and report results#' #' @param modules Optional: specific modules to test. NULL = all.#' @param stop_on_failure If TRUE, stop at first failure#' @return Summary of all test resultsrun_all_golden_tests <- function(modules = NULL, stop_on_failure = FALSE) {    source("tests/golden/golden_test_utils.R")    golden_root <- "tests/golden"    # Find all modules with golden tests  if (is.null(modules)) {    modules <- list.dirs(golden_root, recursive = FALSE, full.names = FALSE)    modules <- modules[modules != ""]  # Remove empty  }    results <- list()  pass_count <- 0  fail_count <- 0    cat("\n========== GOLDEN DATASET TESTS ==========\n\n")    for (module in modules) {    module_dir <- file.path(golden_root, module)    if (!dir.exists(module_dir)) next        projects <- list.dirs(module_dir, recursive = FALSE, full.names = FALSE)        for (project in projects) {      test_name <- sprintf("%s/%s", module, project)      cat(sprintf("Testing: %s ... ", test_name))            result <- tryCatch(        run_golden_test(module, project),        error = function(e) {          list(status = "ERROR", message = e$message)        }      )            results[[test_name]] <- result            if (result$status == "PASS") {        cat("PASS\n")        pass_count <- pass_count + 1      } else {        cat(sprintf("FAIL: %s\n", result$message))        fail_count <- fail_count + 1                if (stop_on_failure) {          cat("\nStopping on first failure.\n")          break        }      }    }        if (stop_on_failure && fail_count > 0) break  }    # Summary  cat("\n==========================================\n")  cat(sprintf("TOTAL: %d passed, %d failed\n", pass_count, fail_count))  cat("==========================================\n\n")    return(list(    pass = pass_count,    fail = fail_count,    results = results  ))}2.5 Creating Golden Tests from Existing ProjectsHelper script: tests/golden/create_golden_from_project.R#' Interactive helper to create a golden test from an existing project#' #' @param project_path Path to project directory with data and config#' @param module Which module to test#' @param project_name Name for this golden testcreate_golden_from_project <- function(project_path, module, project_name) {    source("tests/golden/golden_test_utils.R")    cat(sprintf("Creating golden test: %s/%s\n", module, project_name))  cat(sprintf("From project: %s\n\n", project_path))    # Find data file  data_files <- list.files(project_path, pattern = "\\.(csv|xlsx|parquet)$",                            full.names = TRUE)  cat("Found data files:\n")  for (i in seq_along(data_files)) {    cat(sprintf("  %d: %s\n", i, basename(data_files[i])))  }    data_choice <- as.integer(readline("Select data file number: "))  data_path <- data_files[data_choice]    # Find config file  config_files <- list.files(project_path, pattern = "config.*\\.xlsx$",                              full.names = TRUE)  cat("\nFound config files:\n")  for (i in seq_along(config_files)) {    cat(sprintf("  %d: %s\n", i, basename(config_files[i])))  }    config_choice <- as.integer(readline("Select config file number: "))  config_path <- config_files[config_choice]    # Load data  cat("\nLoading data...\n")  if (grepl("\\.csv$", data_path)) {    input_data <- data.table::fread(data_path)  } else if (grepl("\\.xlsx$", data_path)) {    input_data <- openxlsx::read.xlsx(data_path)  } else if (grepl("\\.parquet$", data_path)) {    input_data <- arrow::read_parquet(data_path)  }    cat(sprintf("Loaded: %d rows, %d columns\n", nrow(input_data), ncol(input_data)))    # Run module  cat(sprintf("\nRunning %s module...\n", module))  output <- run_module(module, input_data, config_path)    if (output$status == "REFUSED") {    cat(sprintf("Module refused: %s\n", output$message))    return(invisible(NULL))  }    # Confirm output looks correct  cat("\nPlease verify the output is correct before capturing as golden.\n")  cat("Output status:", output$status, "\n")    confirm <- readline("Capture this as golden? (yes/no): ")    if (tolower(confirm) == "yes") {    notes <- readline("Any notes for this test case? (or Enter to skip): ")    if (notes == "") notes <- NULL        capture_golden(      module = module,      project_name = project_name,      input_data = input_data,      input_config = config_path,      output = output,      notes = notes    )        cat("\nGolden test created successfully.\n")  } else {    cat("\nCancelled.\n")  }}2.6 Dependencies# Add to renv (likely already have most of these)renv::install("waldo")      # For detailed comparisonsrenv::install("jsonlite")   # For metadata2.7 UsageCapturing a new golden test:source("tests/golden/create_golden_from_project.R")create_golden_from_project(  project_path = "examples/tabs/basic",  module = "tabs",  project_name = "basic_crosstabs")Running all golden tests:source("tests/golden/run_all_golden_tests.R")results <- run_all_golden_tests()Running tests for specific module:results <- run_all_golden_tests(modules = c("tabs", "weighting"))3. Implementation SequenceWeek 1: Parquet CachingDay	Task1	Add arrow package, create parquet_cache.R and schema_manifest.R2	Create load_data_cached wrapper function3	Test with one module (tabs) - verify roundtrip preserves types4	Test with problematic data (factors, dates, edge cases)5	Document, commitWeek 2: Golden Test InfrastructureDay	Task1	Create tests/golden directory structure and core utilities2	Create first golden test manually (tabs with known project)3	Test run_golden_test and compare_to_golden functions4	Create helper script for easy golden capture5	Document, commitWeek 3: Populate Golden TestsDay	Task1-2	Create 2-3 golden tests for tabs (different complexities)3	Create 1-2 golden tests for weighting4	Create 1 golden test each for maxdiff, keydriver5	Run full test suite, fix any issues4. Success CriteriaParquet Caching Factors survive roundtrip (write ? read ? same levels) Dates survive roundtrip Existing CSV cache loading still works At least 2x faster load times on large files Schema warnings appear when mismatches detectedGolden Testing Can capture golden test from any module output Can run all golden tests with single command Clear pass/fail reporting At least 2 golden tests per priority module (tabs, weighting, maxdiff, keydriver) run_all_golden_tests completes without errors5. What This Gives YouAfter completing this:Data loading is robust - No more "why did my factors become characters?"Changes are safe - Run golden tests before and after any modificationClient confidence - You can verify output correctness before deliveryEdge case discovery - Golden tests will surface problems before clients doFoundation for more tests - Easy to add new golden cases as you encounter new project types6. What This Doesn't DoDoesn't change any module logicDoesn't change any existing interfacesDoesn't require packageizationDoesn't affect how you currently workDoesn't require learning new frameworksIt's purely additive protection.Shall I refine any section, or is this ready for you to work on in the background?