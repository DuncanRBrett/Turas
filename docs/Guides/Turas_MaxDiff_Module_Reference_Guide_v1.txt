TURAS MAXDIFF MODULEReference Guide: Best–Worst Scaling and the Responsible Measurement of Relative ImportancePurposeThis document is an authoritative reference for the MaxDiff (Best–Worst Scaling) module in the Turas analytics system. It explains when MaxDiff is appropriate, what problem it solves, how the underlying models work, why Turas has chosen its modelling approach, what alternative approaches exist, and how MaxDiff results should be interpreted responsibly. This is not a user manual and does not describe operational steps.1. What the MaxDiff Module Exists to DoMaxDiff, also known as Best–Worst Scaling, is designed to measure relative importance when direct rating scales are unreliable, inflated, or lack discrimination. It answers questions such as: “Which of these items matter more, relative to the others?”MaxDiff does not measure absolute importance, satisfaction, or performance. It produces a forced-choice ranking of items on a common relative scale.2. When MaxDiff Is the Right ToolMaxDiff is appropriate when:- Many items must be prioritised and rating scales would produce uniformly high scores.- Respondents struggle to meaningfully discriminate on Likert scales.- Relative trade-offs matter more than absolute levels.- Scale-use bias is a concern.- The goal is prioritisation rather than description.Typical use cases include feature prioritisation, message testing, value proposition development, and attribute importance studies.3. When MaxDiff Is the Wrong ToolMaxDiff should not be used when:- Absolute performance or satisfaction must be measured.- Items are not conceptually comparable.- Respondents lack sufficient familiarity with the items.- The list of items is poorly defined or heterogeneous.- Sample size is too small for stable estimation.Using MaxDiff to replace satisfaction or performance measurement is a common misuse.4. How MaxDiff Works ConceptuallyIn a MaxDiff task, respondents are shown small subsets of items and asked to select the most and least important (or appealing) item in each set.Across many tasks and respondents, a pattern of choices emerges. Items selected more often as “best” and less often as “worst” are inferred to have higher relative importance.Critically, MaxDiff produces a zero-sum scale: importance is always relative to the other items included.5. Experimental Design ConsiderationsMaxDiff relies on an experimental design that controls:- how often each item appears,- which items appear together,- and the balance of comparisons.Poor designs produce biased results regardless of modelling sophistication. Turas assumes designs are either externally generated or validated before analysis.6. The Turas Modelling ApproachTuras supports two conceptual modelling approaches to MaxDiff:6.1 Aggregate (Multinomial Logit) ModelsAt the aggregate level, MaxDiff choices are modelled using multinomial logit assumptions. This yields relative utilities for each item at the population level.Strengths:- Transparent and explainable.- Lower computational cost.- Suitable for straightforward prioritisation.Limitations:- Assumes homogeneous preferences.- Does not capture individual-level heterogeneity.6.2 Hierarchical Bayes (HB) ModelsHierarchical Bayes models estimate individual-level utilities while borrowing strength from the population.Strengths:- Captures heterogeneity in preferences.- Produces individual-level scores suitable for segmentation.- More stable in sparse choice designs.Limitations:- More complex.- Computationally intensive.- Requires careful diagnostics.Turas positions HB as the preferred approach for serious prioritisation work, with aggregate models available for simpler applications.7. Alternative Approaches to Importance MeasurementMaxDiff should be understood in relation to other methods:- Rating scales: simple but prone to scale inflation.- Ranking questions: cognitively difficult with many items.- Paired comparison: similar logic but inefficient at scale.- Key Driver Analysis: measures association, not pure importance.Each method answers a different question; MaxDiff is not a universal replacement.8. Which Software Uses Which MaxDiff ModelsCommon software approaches include:- Sawtooth Software: Hierarchical Bayes MaxDiff (industry standard).- Displayr / Q: Aggregate logit and HB variants.- SPSS: Limited native support; often requires custom modelling.- R ecosystems: support via packages such as support.BWS, bayesm, and custom Stan-based implementations.Turas aligns with established methodological practice rather than proprietary black boxes.9. R Packages Used in the Turas MaxDiff ModuleTuras MaxDiff relies on established R packages:- stats and nnet for aggregate multinomial logit modelling.- bayesm, rstanarm, or equivalent for Hierarchical Bayes estimation (depending on configuration).- base R for data handling.- dplyr and tidyr (selectively) for reshaping choice data.- rlang and glue for structured errors and messaging.- openxlsx for deterministic Excel I/O.Turas adds value through governance, design validation, and interpretive safeguards.10. Interpreting MaxDiff Results ResponsiblyMaxDiff outputs should be interpreted as relative preferences within the tested set.Key cautions:- Scores are not absolute.- Differences are only meaningful relative to other items.- Removing or adding items changes the scale.- Small differences may not be practically meaningful.Importance tiers are often more appropriate than exact ranks.11. Reporting MaxDiff ResultsGood reporting:- Emphasises relative ordering, not false precision.- Uses tiers or bands.- Clearly states the item universe.- Avoids translating importance directly into budget shares.Poor reporting treats MaxDiff scores as absolute truths or performance metrics.12. How This Guide Should Be UsedThis guide should be consulted when deciding whether MaxDiff is appropriate, choosing between aggregate and HB models, interpreting outputs, and explaining methodological choices. Procedural details belong in the user manual.Final PerspectiveMaxDiff is one of the most powerful tools for prioritisation in market research. Used well, it produces clear, defensible trade-offs. Used poorly, it creates false certainty. The Turas MaxDiff module exists to ensure methodological discipline and transparency in relative importance measurement.